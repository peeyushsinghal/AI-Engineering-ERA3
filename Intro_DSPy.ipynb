{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peeyushsinghal/AI-Engineering-ERA3/blob/main/Intro_DSPy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e1de941",
      "metadata": {
        "id": "5e1de941"
      },
      "source": [
        "\n",
        "# 🧪 Generative AI in Industry — Maps + FMCG with DSPy & Agentic Pipelines\n",
        "\n",
        "\n",
        "**Audience:** Beginners / Practitioners who want practical exposure to Generative AI + Agentic AI  \n",
        "**Focus:** **DSPy** and Industry Use Cases of Maps, FMCG\n",
        "\n",
        "> Tip: Run cells top-to-bottom. Sections are independent; you can skip/install only what's needed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7014a4cb",
      "metadata": {
        "id": "7014a4cb"
      },
      "source": [
        "\n",
        "## 0) Environment Setup\n",
        "\n",
        "This workshop uses:\n",
        "- Python 3.10+\n",
        "- `dspy` (or `dspy-ai`) for programmable, optimizable LLM pipelines\n",
        "- `pandas` for data handling\n",
        "- `rapidfuzz` for string similarity\n",
        "- An LLM provider (Gemini or OpenAI or compatible).\n",
        "\n",
        "> If you don't have Internet in your environment, skip installs and read through the code; it will still serve as a template.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If your environment allows, uncomment to install.\n",
        "!pip install --quiet dspy-ai rapidfuzz pandas python-dotenv\n",
        "# For Google API:\n",
        "!pip install --quiet google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMoKJUEbudNI",
        "outputId": "ea897405-9874-4092-8a73-5995fae3eacc"
      },
      "id": "gMoKJUEbudNI",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.1/260.1 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "tCBHGYLiu0Iu"
      },
      "id": "tCBHGYLiu0Iu",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = Path('data')\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"Setup complete. Data directory:\", DATA_DIR.resolve())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXOGXkZhu51I",
        "outputId": "8bfc731c-1a85-4a05-de38-051b7a27d05f"
      },
      "id": "PXOGXkZhu51I",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Data directory: /content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ee5191a",
      "metadata": {
        "id": "3ee5191a"
      },
      "source": [
        "\n",
        "### 0.1) Configure Model / API Keys\n",
        "\n",
        "You can use **Google Gemini** or **OpenAI** or any provider supported by DSPy\n",
        "Set the env var(s) appropriately, then initialize the DSPy model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "892d62ab",
      "metadata": {
        "id": "892d62ab"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import dspy\n",
        "\n",
        "# Configure API Key\n",
        "GEMINI_API_KEY = \"AIzaSyAh0Kp5YuOCTmc5qKNo0R5cWzWWGe8x_OQ\"\n",
        "GEMINI_MODEL = \"gemini/gemini-2.5-flash\" # gemini-2.0-flash"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa415e52",
      "metadata": {
        "id": "fa415e52"
      },
      "source": [
        "\n",
        "### 0.2) Initialize DSPy with your chosen language model\n",
        "\n",
        "Let's set up a simple LLM in DSPy. **Fill in the code cell below to:**\n",
        "- Import dspy\n",
        "- Set up a language model (you can use a placeholder for the API key)\n",
        "- Configure dspy to use this LLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a30d9b01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a30d9b01",
        "outputId": "8558744b-8669-42a4-9fc5-9943af0085f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DSPy initialized with model: gemini/gemini-2.5-flash\n"
          ]
        }
      ],
      "source": [
        "# If DSPy is installed, this will work. Otherwise, treat as reference code.\n",
        "try:\n",
        "    import dspy\n",
        "    # Initialize a Gemini-based LM for DSPy (e.g., Gemini-2.5-flash)\n",
        "    llm = dspy.LM(\n",
        "        model= GEMINI_MODEL,\n",
        "        api_key=GEMINI_API_KEY\n",
        "    )\n",
        "    dspy.settings.configure(lm=llm)\n",
        "    print(\"DSPy initialized with model:\", GEMINI_MODEL)\n",
        "except Exception as e:\n",
        "    print(\"DSPy not available or failed to initialize:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try Calling the LLM\n",
        "\n",
        "Write a code cell to call the LLM with a simple prompt, e.g., 'Say this is a test!'.\n",
        "\n",
        "Note! If this does not work, most likely something is wrong with the setup of your LLM."
      ],
      "metadata": {
        "id": "t-jLqUtWhBSu"
      },
      "id": "t-jLqUtWhBSu"
    },
    {
      "cell_type": "code",
      "source": [
        "llm(\"Say: this is a test!\", temperature=0.7)  # => ['This is a test!']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq2FVJ-lg6fA",
        "outputId": "1882bad8-2c65-4842-c3f4-46f6be7e4c7e"
      },
      "id": "cq2FVJ-lg6fA",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This is a test!']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also use the traditional role format: messages=\n",
        "[{\"role\": \"user\", \"content\": \"Say this is not a test!\"}]\n",
        "Try it here."
      ],
      "metadata": {
        "id": "i2mlWdwihZdv"
      },
      "id": "i2mlWdwihZdv"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Call the LLM with the messages format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hd4FIxKShc7x",
        "outputId": "2880b899-cf87-4588-c15f-5b7ddc5fc45f"
      },
      "id": "hd4FIxKShc7x",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This is not a test!']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Click to show solution</summary>\n",
        "\n",
        "```python\n",
        "llm(messages=[{\"role\": \"user\", \"content\": \"Say this is not a test!\"}])  # => ['This is not a test!']\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "qujfzRI9hgkx"
      },
      "id": "qujfzRI9hgkx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DSPy Signatures and Modules\n",
        "\n",
        "**Exercise:** Define a simple DSPy signature for sentiment classification.\n",
        "\n",
        "- Create a class `Classify` inheriting from `dspy.Signature`\n",
        "- Add input and output fields for sentence, sentiment, and confidence\n",
        "- Instantiate a Predict module and use it on a sample sentence"
      ],
      "metadata": {
        "id": "coTovYW-h74_"
      },
      "id": "coTovYW-h74_"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "class Classify(dspy.Signature):\n",
        "    \"\"\"Classify sentiment of a given sentence.\"\"\"\n",
        "\n",
        "    sentence: str = dspy.InputField()\n",
        "    sentiment: Literal[\"positive\", \"negative\", \"neutral\"] = dspy.OutputField()\n",
        "    confidence: float = dspy.OutputField()\n",
        "\n",
        "classify = dspy.Predict(Classify)\n",
        "classify(sentence=\"This book was super fun to read, though not the last chapter.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQEI8q9GiAVz",
        "outputId": "8a74a489-fc5e-493f-b39d-59f8c81fbd4d"
      },
      "id": "bQEI8q9GiAVz",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prediction(\n",
              "    sentiment='positive',\n",
              "    confidence=0.75\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classify.history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBlBbevXin_B",
        "outputId": "d43d7a1e-3095-4570-e4a4-66e776bea04a"
      },
      "id": "dBlBbevXin_B",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'prompt': None,\n",
              "  'messages': [{'role': 'system',\n",
              "    'content': \"Your input fields are:\\n1. `sentence` (str):\\nYour output fields are:\\n1. `sentiment` (Literal['positive', 'negative', 'neutral']): \\n2. `confidence` (float):\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## sentence ## ]]\\n{sentence}\\n\\n[[ ## sentiment ## ]]\\n{sentiment}        # note: the value you produce must exactly match (no extra characters) one of: positive; negative; neutral\\n\\n[[ ## confidence ## ]]\\n{confidence}        # note: the value you produce must be a single float value\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Classify sentiment of a given sentence.\"},\n",
              "   {'role': 'user',\n",
              "    'content': \"[[ ## sentence ## ]]\\nThis book was super fun to read, though not the last chapter.\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## sentiment ## ]]` (must be formatted as a valid Python Literal['positive', 'negative', 'neutral']), then `[[ ## confidence ## ]]` (must be formatted as a valid Python float), and then ending with the marker for `[[ ## completed ## ]]`.\"}],\n",
              "  'kwargs': {},\n",
              "  'response': ModelResponse(id='Am2saLbkJ7Cc-8YPw6CtwAs', created=1756130558, model='gemini-2.5-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## sentiment ## ]]\\npositive\\n\\n[[ ## confidence ## ]]\\n0.75\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=508, prompt_tokens=251, total_tokens=759, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=484, rejected_prediction_tokens=None, text_tokens=24), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=251, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[], cache_hit=None),\n",
              "  'outputs': ['[[ ## sentiment ## ]]\\npositive\\n\\n[[ ## confidence ## ]]\\n0.75\\n\\n[[ ## completed ## ]]'],\n",
              "  'usage': {'completion_tokens': 508,\n",
              "   'prompt_tokens': 251,\n",
              "   'total_tokens': 759,\n",
              "   'completion_tokens_details': CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=484, rejected_prediction_tokens=None, text_tokens=24),\n",
              "   'prompt_tokens_details': PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=251, image_tokens=None)},\n",
              "  'cost': 0.0013453,\n",
              "  'timestamp': '2025-08-25T14:02:42.665811',\n",
              "  'uuid': 'ff749fda-f705-4fbe-be9e-1f440b9d3601',\n",
              "  'model': 'gemini/gemini-2.5-flash',\n",
              "  'response_model': 'gemini-2.5-flash',\n",
              "  'model_type': 'chat'}]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classify.inspect_history()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MRql2tpief4",
        "outputId": "84367659-d960-4292-90ba-fe7350168189"
      },
      "id": "_MRql2tpief4",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[34m[2025-08-25T14:02:42.665811]\u001b[0m\n",
            "\n",
            "\u001b[31mSystem message:\u001b[0m\n",
            "\n",
            "Your input fields are:\n",
            "1. `sentence` (str):\n",
            "Your output fields are:\n",
            "1. `sentiment` (Literal['positive', 'negative', 'neutral']): \n",
            "2. `confidence` (float):\n",
            "All interactions will be structured in the following way, with the appropriate values filled in.\n",
            "\n",
            "[[ ## sentence ## ]]\n",
            "{sentence}\n",
            "\n",
            "[[ ## sentiment ## ]]\n",
            "{sentiment}        # note: the value you produce must exactly match (no extra characters) one of: positive; negative; neutral\n",
            "\n",
            "[[ ## confidence ## ]]\n",
            "{confidence}        # note: the value you produce must be a single float value\n",
            "\n",
            "[[ ## completed ## ]]\n",
            "In adhering to this structure, your objective is: \n",
            "        Classify sentiment of a given sentence.\n",
            "\n",
            "\n",
            "\u001b[31mUser message:\u001b[0m\n",
            "\n",
            "[[ ## sentence ## ]]\n",
            "This book was super fun to read, though not the last chapter.\n",
            "\n",
            "Respond with the corresponding output fields, starting with the field `[[ ## sentiment ## ]]` (must be formatted as a valid Python Literal['positive', 'negative', 'neutral']), then `[[ ## confidence ## ]]` (must be formatted as a valid Python float), and then ending with the marker for `[[ ## completed ## ]]`.\n",
            "\n",
            "\n",
            "\u001b[31mResponse:\u001b[0m\n",
            "\n",
            "\u001b[32m[[ ## sentiment ## ]]\n",
            "positive\n",
            "\n",
            "[[ ## confidence ## ]]\n",
            "0.75\n",
            "\n",
            "[[ ## completed ## ]]\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dd43331",
      "metadata": {
        "id": "5dd43331"
      },
      "source": [
        "\n",
        "---\n",
        "## 1) Warm-Up (GenAI Basics): Product Description Generator (FMCG)\n",
        "\n",
        "**Goal:** See how style, tone, and temperature affect outputs.\n",
        "\n",
        "**Task:** Given a product name & features, generate a short marketing description in 3 tones.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Different Tones\n",
        "product = \"Sunburst Orange Juice\"\n",
        "tones = [\"Formal\", \"Casual\", \"Punchy / Ad-like\"]"
      ],
      "metadata": {
        "id": "2mby186jxOkA"
      },
      "id": "2mby186jxOkA",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Define signature\n",
        "class ProductDescription(dspy.Signature):\n",
        "    \"\"\"Generate a product description given tone and product.\"\"\"\n",
        "    tone = dspy.InputField()\n",
        "    product = dspy.InputField()\n",
        "    description = dspy.OutputField()\n",
        "\n",
        "# Predict module\n",
        "gen = dspy.Predict(ProductDescription)\n",
        "\n",
        "for tone in tones:\n",
        "    result = gen(tone=tone, product=product)\n",
        "    print(f\"\\n--- {tone} ---\\n{result.description}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rGLamg8zcly",
        "outputId": "7768d097-2245-4b6a-e134-7426e93310e2"
      },
      "id": "-rGLamg8zcly",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Formal ---\n",
            "We proudly present Sunburst Orange Juice, a distinguished beverage crafted from the finest, sun-ripened oranges. Each serving offers a meticulously balanced profile of natural sweetness and invigorating tang, designed to provide a refreshing and revitalizing experience. Our commitment to quality ensures that every glass delivers the pure essence of premium citrus, making Sunburst Orange Juice an exemplary choice for discerning palates seeking both exquisite taste and wholesome refreshment.\n",
            "\n",
            "--- Casual ---\n",
            "Hey there, looking for a little pick-me-up? Grab a glass of Sunburst Orange Juice! It's super refreshing, bursting with that classic, sunny orange flavor you love. Perfect for breakfast, a midday boost, or just chilling out. Seriously, it's like sunshine in a bottle – you can't go wrong!\n",
            "\n",
            "--- Punchy / Ad-like ---\n",
            "Tired of the same old? Crave a burst of pure sunshine? Grab Sunburst Orange Juice! We're talking 100% pure, squeezed-from-the-source, vibrant orange goodness. Every sip is a wake-up call, a flavor explosion that electrifies your taste buds and powers your day. Forget dull, embrace the dazzling. Get your glow on with Sunburst – the only juice that truly shines!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***No writing prompts***"
      ],
      "metadata": {
        "id": "Tz-ZS2dL0QA4"
      },
      "id": "Tz-ZS2dL0QA4"
    },
    {
      "cell_type": "code",
      "source": [
        "gen.inspect_history()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciLCW4m70TYE",
        "outputId": "0c937b84-0051-40fc-8623-0aeaed61ad38"
      },
      "id": "ciLCW4m70TYE",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[34m[2025-08-24T06:41:54.304390]\u001b[0m\n",
            "\n",
            "\u001b[31mSystem message:\u001b[0m\n",
            "\n",
            "Your input fields are:\n",
            "1. `tone` (str): \n",
            "2. `product` (str):\n",
            "Your output fields are:\n",
            "1. `description` (str):\n",
            "All interactions will be structured in the following way, with the appropriate values filled in.\n",
            "\n",
            "[[ ## tone ## ]]\n",
            "{tone}\n",
            "\n",
            "[[ ## product ## ]]\n",
            "{product}\n",
            "\n",
            "[[ ## description ## ]]\n",
            "{description}\n",
            "\n",
            "[[ ## completed ## ]]\n",
            "In adhering to this structure, your objective is: \n",
            "        Generate a product description given tone and product.\n",
            "\n",
            "\n",
            "\u001b[31mUser message:\u001b[0m\n",
            "\n",
            "[[ ## tone ## ]]\n",
            "Punchy / Ad-like\n",
            "\n",
            "[[ ## product ## ]]\n",
            "SunBurst Orange Juice\n",
            "\n",
            "Respond with the corresponding output fields, starting with the field `[[ ## description ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
            "\n",
            "\n",
            "\u001b[31mResponse:\u001b[0m\n",
            "\n",
            "\u001b[32m[[ ## description ## ]]\n",
            "Tired of the same old? Ignite your day with SunBurst Orange Juice! We're talking pure, unadulterated sunshine in a bottle. Each sip is a vibrant explosion of fresh-squeezed, zesty orange that'll wake up your taste buds and power up your morning. Ditch the dull, grab the glow. Get your SunBurst today – taste the bright side!\n",
            "\n",
            "[[ ## completed ## ]]\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "808942c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "808942c0",
        "outputId": "7303221c-720b-4c4f-fc64-4d6b67d3ae92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DSPy not available; here's the prompt you can try with your LLM:\n",
            "\n",
            "You are a marketing copywriter. Write a concise, compelling product description\n",
            "for the FMCG product below in 3 distinct tones: (1) formal, (2) casual, (3) ad-like punchy.\n",
            "Each variant should be 2-3 sentences max.\n",
            "\n",
            "Product: SunBurst Orange Juice\n",
            "Key features: No added sugar, 100% pure, Rich in Vitamin C, Cold-pressed\n",
            "Target audience: health-conscious millennials\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_fields.py:198: UserWarning: Field name \"instructions\" in \"StringSignature\" shadows an attribute in parent \"Signature\"\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "product = {\n",
        "    \"name\": \"SunBurst Orange Juice\",\n",
        "    \"features\": [\"No added sugar\", \"100% pure\", \"Rich in Vitamin C\", \"Cold-pressed\"],\n",
        "    \"audience\": \"health-conscious millennials\"\n",
        "}\n",
        "\n",
        "prompt = f'''\n",
        "You are a marketing copywriter. Write a concise, compelling product description\n",
        "for the FMCG product below in 3 distinct tones: (1) formal, (2) casual, (3) ad-like punchy.\n",
        "Each variant should be 2-3 sentences max.\n",
        "\n",
        "Product: {product[\"name\"]}\n",
        "Key features: {\", \".join(product[\"features\"])}\n",
        "Target audience: {product[\"audience\"]}\n",
        "'''\n",
        "\n",
        "try:\n",
        "    import dspy\n",
        "    gen = dspy.Predict(\"instructions -> descriptions\")\n",
        "    out = gen(instructions=prompt).descriptions\n",
        "    print(out)\n",
        "except Exception:\n",
        "    # Fallback if DSPy isn't available: print the prompt for reference.\n",
        "    print(\"DSPy not available; here's the prompt you can try with your LLM:\")\n",
        "    print(prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f31d6e45",
      "metadata": {
        "id": "f31d6e45"
      },
      "source": [
        "\n",
        "---\n",
        "## 2) Maps Mini-Project: Street Name Normalization & Matching\n",
        "\n",
        "**Problem:** Real-world street names vary (`\"MG Road\"`, `\"M.G. Rd\"`, `\"Mahatma Gandhi Road\"`).  \n",
        "**Goal:** Normalize variants to a canonical form and match duplicates.\n",
        "\n",
        "We'll combine:\n",
        "- **LLM-based normalization** (expand abbreviations, fix casing, remove punctuation)\n",
        "- **String similarity** via `rapidfuzz` for robust matching\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aca4285d",
      "metadata": {
        "id": "aca4285d"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from rapidfuzz import fuzz, process\n",
        "\n",
        "# Sample data with variants\n",
        "streets = pd.DataFrame({\n",
        "    \"raw_street\": [\n",
        "        \"MG Road\", \"M.G. Rd\", \"Mahatma Gandhi Rd\", \"Mahatma Gandhi Road\",\n",
        "        \"St John's Rd\", \"Saint Johns Road\", \"St. John’s Rd\", \"St. John Road\",\n",
        "        \"Nehru Marg\", \"Jawaharlal Nehru Marg\", \"J L Nehru Marg\",\n",
        "        \"Ring Rd\", \"Outer Ring Road\", \"Outer Rng Rd\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "streets.to_csv(\"data/streets_raw.csv\", index=False)\n",
        "streets.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9e043d0",
      "metadata": {
        "id": "e9e043d0"
      },
      "source": [
        "\n",
        "### 2.1) LLM Normalizer (DSPy)\n",
        "\n",
        "We'll create a simple **Signature** and **Predictor** that maps a raw street name → canonical normalized name.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a8cfa54",
      "metadata": {
        "id": "2a8cfa54"
      },
      "outputs": [],
      "source": [
        "\n",
        "normalizer_spec = \"\"\"\n",
        "Given an Indian street name variant, return a clean, canonical, expanded form:\n",
        "- Expand common abbreviations (e.g., 'Rd' → 'Road', 'St' → 'Saint' when it's a person's name; else 'Street' if context suggests)\n",
        "- Remove unnecessary punctuation\n",
        "- Use Title Case\n",
        "- Prefer full names (e.g., 'MG' → 'Mahatma Gandhi' when unambiguous)\n",
        "Return only the normalized name, no extra text.\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    import dspy\n",
        "\n",
        "    class NormalizeStreet(dspy.Signature):\n",
        "        raw_name = dspy.InputField()\n",
        "        normalized = dspy.OutputField(desc=\"normalized, canonical street name\")\n",
        "\n",
        "    normalize = dspy.Predict(NormalizeStreet)\n",
        "\n",
        "    def llm_normalize(name: str) -> str:\n",
        "        r = normalize(raw_name=f\"{name}\n",
        "\n",
        "Guidelines:\n",
        "{normalizer_spec}\")\n",
        "        return r.normalized.strip()\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"DSPy not available; falling back to a rule-based normalizer:\", e)\n",
        "    import re\n",
        "\n",
        "    ABBR = {\n",
        "        r\"\\brd\\b\": \"Road\",\n",
        "        r\"\\brd.\\b\": \"Road\",\n",
        "        r\"\\bst\\b\": \"Street\",\n",
        "        r\"\\bst.\\b\": \"Street\",\n",
        "        r\"\\bmg\\b\": \"Mahatma Gandhi\",\n",
        "        r\"\\bjl\\b\": \"Jawaharlal\",\n",
        "        r\"\\bmarg\\b\": \"Marg\",\n",
        "        r\"\\brng\\b\": \"Ring\",\n",
        "    }\n",
        "    def rule_normalize(text: str) -> str:\n",
        "        t = text.lower()\n",
        "        for pat, rep in ABBR.items():\n",
        "            t = re.sub(pat, rep.lower(), t)\n",
        "        t = re.sub(r\"[.’']\", \"\", t)\n",
        "        t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "        return t.title()\n",
        "\n",
        "    def llm_normalize(name: str) -> str:\n",
        "        return rule_normalize(name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ddcc682",
      "metadata": {
        "id": "1ddcc682"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.read_csv(\"data/streets_raw.csv\")\n",
        "df[\"normalized\"] = df[\"raw_street\"].apply(llm_normalize)\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d284b300",
      "metadata": {
        "id": "d284b300"
      },
      "source": [
        "\n",
        "### 2.2) Fuzzy Matching to Group Duplicates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0c101ce",
      "metadata": {
        "id": "f0c101ce"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Group streets by similarity of their normalized form\n",
        "# We'll use a simple threshold; in production, tune per locale and evaluate with ground truth.\n",
        "threshold = 90\n",
        "\n",
        "unique_norms = df[\"normalized\"].unique().tolist()\n",
        "clusters = []\n",
        "visited = set()\n",
        "\n",
        "for i, s in enumerate(unique_norms):\n",
        "    if s in visited:\n",
        "        continue\n",
        "    visited.add(s)\n",
        "    # Find close matches\n",
        "    matches = process.extract(s, unique_norms, scorer=fuzz.token_sort_ratio, limit=None)\n",
        "    group = [m[0] for m in matches if m[1] >= threshold]\n",
        "    clusters.append(group)\n",
        "    visited.update(group)\n",
        "\n",
        "# Map each row to a cluster id\n",
        "cluster_map = {}\n",
        "for idx, group in enumerate(clusters):\n",
        "    for g in group:\n",
        "        cluster_map[g] = idx\n",
        "\n",
        "df[\"cluster_id\"] = df[\"normalized\"].map(cluster_map)\n",
        "df.sort_values([\"cluster_id\", \"normalized\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c0a7da9",
      "metadata": {
        "id": "2c0a7da9"
      },
      "source": [
        "\n",
        "**Exercise:** Try changing the `threshold` to see how clusters merge/split.  \n",
        "**Discussion:** When to trust LLM normalization vs rules; human-in-the-loop QA for map data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15b9003d",
      "metadata": {
        "id": "15b9003d"
      },
      "source": [
        "\n",
        "---\n",
        "## 3) FMCG Mini-Project: Reviews → Insights → Actions\n",
        "\n",
        "**Goal:** Generate synthetic reviews for a new product, summarize themes, extract insights, and recommend actions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba3333d9",
      "metadata": {
        "id": "ba3333d9"
      },
      "outputs": [],
      "source": [
        "\n",
        "product = \"SunBurst Orange Juice\"\n",
        "aspects = [\"taste\", \"price\", \"packaging\", \"availability\", \"healthiness\"]\n",
        "\n",
        "try:\n",
        "    import dspy\n",
        "\n",
        "    class ReviewSynth(dspy.Signature):\n",
        "        product = dspy.InputField()\n",
        "        aspects = dspy.InputField()\n",
        "        reviews = dspy.OutputField(desc=\"10 diverse, short customer reviews\")\n",
        "\n",
        "    synth = dspy.Predict(ReviewSynth)\n",
        "    reviews_text = synth(product=product, aspects=aspects).reviews\n",
        "except Exception:\n",
        "    # Fallback: sample static reviews\n",
        "    reviews_text = \"\"\"\n",
        "1) Great taste but a bit pricey.\n",
        "2) Love the no-sugar claim; feels healthy.\n",
        "3) Packaging leaks if kept sideways.\n",
        "4) Hard to find at my local store.\n",
        "5) Kids enjoy it; refreshing and pulpy.\n",
        "6) Price is okay during discounts.\n",
        "7) Wish there was a smaller pack size.\n",
        "8) Tastes natural, not too sweet.\n",
        "9) Outer packaging is attractive.\n",
        "10) Delivery took long; store was out of stock.\n",
        "\"\"\"\n",
        "\n",
        "print(reviews_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a30d95f7",
      "metadata": {
        "id": "a30d95f7"
      },
      "source": [
        "\n",
        "### 3.1) Summarize & Extract Insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a91431ec",
      "metadata": {
        "id": "a91431ec"
      },
      "outputs": [],
      "source": [
        "\n",
        "try:\n",
        "    import dspy\n",
        "\n",
        "    class SummarizeReviews(dspy.Signature):\n",
        "        reviews = dspy.InputField()\n",
        "        summary = dspy.OutputField(desc=\"pros, cons, notable quotes\")\n",
        "\n",
        "    class ExtractInsights(dspy.Signature):\n",
        "        summary = dspy.InputField()\n",
        "        insights = dspy.OutputField(desc=\"3-5 crisp insights with evidence\")\n",
        "\n",
        "    summarize = dspy.ChainOfThought(SummarizeReviews)\n",
        "    extract = dspy.Predict(ExtractInsights)\n",
        "\n",
        "    summary = summarize(reviews=reviews_text).summary\n",
        "    insights = extract(summary=summary).insights\n",
        "\n",
        "    print(\"SUMMARY:\\n\", summary)\n",
        "    print(\"\\nINSIGHTS:\\n\", insights)\n",
        "\n",
        "except Exception:\n",
        "    print(\"DSPy not available; here is a template prompt you can run with your LLM:\")\n",
        "    print(\"\"\"\n",
        "Summarize the following reviews into pros, cons, and notable quotes. Then provide 3-5 crisp insights:\n",
        "\"\"\")\n",
        "    print(reviews_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94059490",
      "metadata": {
        "id": "94059490"
      },
      "source": [
        "\n",
        "---\n",
        "## 4) Agentic AI with DSPy: Compose a Pipeline\n",
        "\n",
        "We'll build a 3-stage pipeline:\n",
        "1. **Summarizer** – condense reviews/sales text\n",
        "2. **Insight Generator** – extract trends/causes\n",
        "3. **Recommender** – propose next actions (pricing, packaging, distribution, marketing)\n",
        "\n",
        "You'll see: how **modules** wrap LLM calls, how to **swap models**, and how to **optimize prompts**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0f0f437",
      "metadata": {
        "id": "f0f0f437"
      },
      "outputs": [],
      "source": [
        "\n",
        "try:\n",
        "    import dspy\n",
        "\n",
        "    class Summarizer(dspy.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            class Sig(dspy.Signature):\n",
        "                text = dspy.InputField()\n",
        "                summary = dspy.OutputField()\n",
        "            self.step = dspy.ChainOfThought(Sig)\n",
        "        def forward(self, text):\n",
        "            return self.step(text=text).summary\n",
        "\n",
        "    class InsightGen(dspy.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            class Sig(dspy.Signature):\n",
        "                summary = dspy.InputField()\n",
        "                insights = dspy.OutputField()\n",
        "            self.step = dspy.Predict(Sig)\n",
        "        def forward(self, summary):\n",
        "            return self.step(summary=summary).insights\n",
        "\n",
        "    class Recommender(dspy.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            class Sig(dspy.Signature):\n",
        "                insights = dspy.InputField()\n",
        "                actions = dspy.OutputField()\n",
        "            self.step = dspy.Predict(Sig)\n",
        "        def forward(self, insights):\n",
        "            return self.step(insights=insights).actions\n",
        "\n",
        "    class FMCGPipeline(dspy.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.summarizer = Summarizer()\n",
        "            self.insightgen = InsightGen()\n",
        "            self.recommender = Recommender()\n",
        "\n",
        "        def forward(self, text):\n",
        "            summary = self.summarizer(text=text)\n",
        "            insights = self.insightgen(summary=summary)\n",
        "            actions = self.recommender(insights=insights)\n",
        "            return dict(summary=summary, insights=insights, actions=actions)\n",
        "\n",
        "    pipeline = FMCGPipeline()\n",
        "\n",
        "    sample_text = reviews_text\n",
        "    result = pipeline(text=sample_text)\n",
        "    print(\"SUMMARY:\\n\", result[\"summary\"])\n",
        "    print(\"\\nINSIGHTS:\\n\", result[\"insights\"])\n",
        "    print(\"\\nACTIONS:\\n\", result[\"actions\"])\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"DSPy not available; here is the logical flow you can implement with any LLM:\")\n",
        "    print(\"1) Summarize -> 2) Extract Insights -> 3) Recommend Actions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93fe85a9",
      "metadata": {
        "id": "93fe85a9"
      },
      "source": [
        "\n",
        "### 4.1) (Optional) DSPy Optimization\n",
        "\n",
        "DSPy supports **teleprompter**-style optimization given labeled examples.  \n",
        "Below is a minimal sketch (fill `train_data` with (input, target) pairs).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b7e5e21",
      "metadata": {
        "id": "5b7e5e21"
      },
      "outputs": [],
      "source": [
        "\n",
        "try:\n",
        "    import dspy\n",
        "\n",
        "    # Minimal demo dataset (toy). Replace with real (input, target) pairs.\n",
        "    train_data = [\n",
        "        dict(text=\"Pricey but delicious. Hard to find locally.\", target_actions=\"Run local availability campaign; limited-time discount\"),\n",
        "        dict(text=\"Leaky packaging. Love the no sugar.\", target_actions=\"Improve cap seal; emphasize health benefit in ads\"),\n",
        "    ]\n",
        "\n",
        "    class ActionsTeacher(dspy.Signature):\n",
        "        text = dspy.InputField()\n",
        "        actions = dspy.OutputField()\n",
        "\n",
        "    # A tiny trainer that pretends \"actions\" is the supervised target.\n",
        "    class TinyTrainer(dspy.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.pipeline = FMCGPipeline()\n",
        "        def forward(self, text):\n",
        "            out = self.pipeline(text=text)\n",
        "            return out[\"actions\"]\n",
        "\n",
        "    # In real usage, use dspy.teleprompt.BootstrapFewShot or similar.\n",
        "    # Here we simply run the pipeline on training data as illustration.\n",
        "    trainer = TinyTrainer()\n",
        "    for ex in train_data:\n",
        "        _ = trainer(text=ex[\"text\"])\n",
        "    print(\"Optimization sketch complete (replace with DSPy teleprompters in real training).\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Skipping optimization sketch due to:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9df2d0d5",
      "metadata": {
        "id": "9df2d0d5"
      },
      "source": [
        "\n",
        "---\n",
        "## 5) Stretch Goals\n",
        "- Add a **retrieval** step (RAG) for product manuals/FAQs before generating actions.\n",
        "- Use a **validator** module to check if actions are grounded in the summary.\n",
        "- For Maps: add **house-number parsing**, **localization**, and **confidence scoring**.\n",
        "- Log prompts/outputs and build a small **evaluation harness** with golden test cases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53aa3cc5",
      "metadata": {
        "id": "53aa3cc5"
      },
      "source": [
        "\n",
        "---\n",
        "## 6) Troubleshooting\n",
        "\n",
        "- **No Internet?** Skip installs, read through code, and run later on a connected machine.\n",
        "- **API errors?** Check `OPENAI_API_KEY`, `OPENAI_BASE_URL`, and `OPENAI_MODEL` env vars.\n",
        "- **DSPy version mismatch?** Adjust the LM initialization to your version.\n",
        "- **String matching too strict?** Lower the threshold or use another scorer.\n",
        "- **Time check:** Generated on 2025-08-23 02:32:58.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}